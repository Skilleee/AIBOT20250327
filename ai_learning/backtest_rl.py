"""
backtest_rl.py

Backtestar en redan tr√§nad RL-modell (ex. PPO) mot historisk data,
r√§knar ut PnL, etc.
"""

import os
import logging
import pandas as pd
import numpy as np
from stable_baselines3 import PPO
from ai_learning.reinforcement_learning import TradingEnv

# Konfigurera loggning
logging.basicConfig(
    filename="backtest_rl.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def backtest_rl_agent(df: pd.DataFrame, model_path="rl_trading_model.zip"):
    """
    Backtestar en tr√§nad RL-agent p√• historisk data.
    
    Args:
        df (pd.DataFrame): DataFrame med kolumner som "close", "momentum", "volume".
        model_path (str): S√∂kv√§g till den tr√§nade RL-modellen.
    
    Returns:
        dict: {"reward": totalt_reward, "final_value": slutligt_portf√∂ljv√§rde}
    """

    if not isinstance(model_path, str):
        raise ValueError(f"‚ùå Model path m√•ste vara en str√§ng, men fick: {type(model_path)}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"‚ùå RL-modellen hittades inte p√• {model_path}")

    logging.info(f"üì• Laddar RL-modell fr√•n: {os.path.abspath(model_path)}")
    print(f"üì• Laddar RL-modell fr√•n: {os.path.abspath(model_path)}")

    try:
        model = PPO.load(model_path)
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Misslyckades att ladda RL-modellen: {str(e)}")
        raise

    # Initiera TradingEnv
    env = TradingEnv(data=df)

    obs, _ = env.reset()
    done = False
    total_reward = 0.0
    step = 0

    while not done:
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        total_reward += reward

        # Logga varje steg
        current_value = env._get_portfolio_value()
        msg = f"Steg {step:3d} | Action: {action} | Reward: {reward:.2f} | Portf√∂ljv√§rde: {current_value:.2f}"
        logging.info(msg)
        print(msg)
        step += 1

    final_value = env._get_portfolio_value()
    summary = f"‚úÖ Backtest: Totalt reward = {total_reward:.2f}, slutligt portf√∂ljv√§rde = {final_value:.2f}"
    logging.info(summary)
    print(summary)

    return {"reward": total_reward, "final_value": final_value}

if __name__ == "__main__":
    # Exempel: Dummydata f√∂r test
    df = pd.DataFrame({
        "close": [100, 101, 102, 103, 104],
        "momentum": [0.1, 0.2, -0.1, 0.3, -0.2],
        "volume": [500, 400, 600, 550, 500],
    })

    result = backtest_rl_agent(df, model_path="rl_trading_model.zip")
